{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sistema de recomendación: \n",
    "## www.ElResumen.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este decumeno se va a crear un sistema de recomendación de los libros del portal *elresumen.com* haciendo uso del **Procesamiento del Lenguaje Natural** a través de los resúmenes disponibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/sr-books.jpg\" width=\"600px\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las librerías utilizadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rodrigo\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import csv\n",
    "from time import time\n",
    "import numpy as np\n",
    "from gensim import corpora, models, similarities\n",
    "import gensim\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la obtención de estos resumenes vamos a utilizar una técnica conocida como * Web Scraping* que consiste en, navegando a través del código HTML de una página web, extraer cierta información.\n",
    "\n",
    "Para realizar ello lo primero tenemos que hacer es 'escrapear' en la página principal (http://www.elresumen.com/listado_de_libros.htm) todos los enlaces donde se encuentra el resumen de cada libro para posteriormente entrar en dicho enlace y así poder extraer su resumen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos obteniendo los enlaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rodrigo\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\Rodrigo\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "url = \"http://www.elresumen.com/listado_de_libros.htm\"\n",
    "soup = BeautifulSoup(requests.get(url).text)\n",
    "nombre = []\n",
    "lista = soup('tr')\n",
    "link=[]\n",
    "\n",
    "for i in lista:\n",
    "    try:\n",
    "        link.append(i.find('a').get('href'))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "nones = link.count(None)\n",
    "none = [link.remove(None) for k in range(nones)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para sacar información del HTML tenemos que traernos todo el código de la página a la memoria. Esto conlleva un cierto tiempo debido a la gran cantidad de enlaces que tenemos que \"visitar\", por lo que vamos a utilizar la función \"Threads\" de la librería *threading* para paralelizar.\n",
    "\n",
    "Para comenzar tenemos que dividir el problema, en este caso como tenemos una lista con todos los enlaces, vamos a crear varios diccionarios donde en cada uno se encontrará una parte de los enlaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urls = [\"http://www.elresumen.com/{}\".format(l) for l in link]\n",
    "\n",
    "u = len(urls)\n",
    "n_paralelo = u//4\n",
    "\n",
    "threads = list()\n",
    "resu=[]\n",
    "d = dict()\n",
    "for b in range(n_paralelo):\n",
    "    d['urls%1d' % (b+1)] = urls[b*u//(n_paralelo):u*(b+1)//(n_paralelo)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rodrigo\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 882 of the file C:\\Users\\Rodrigo\\Anaconda3\\lib\\threading.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completado: 25.0 por ciento\n",
      "Completado: 50.0 por ciento\n",
      "Completado: 75.0 por ciento\n",
      "Completado: 100.0 por ciento\n",
      "| Tiempo de scrapeo total: 9.0107 s |\n"
     ]
    }
   ],
   "source": [
    "t00 = time()\n",
    "def worker(ur):\n",
    "    w = 0\n",
    "    for h in d[str(ur)]:\n",
    "        soup2 = BeautifulSoup(requests.get(str(h)).text)\n",
    "        re = soup2('main', 'main-content')\n",
    "        resumen = []\n",
    "        [nombre.append(a.find_all('h1')[1].text) for a in re]\n",
    "        [[resumen.append(b.text) for b in a.find_all('p') if len(str(b)) > 100]\n",
    "         for a in re]\n",
    "    \n",
    "        resumen.remove(resumen[len(resumen)-1])\n",
    "        resumen.remove(resumen[len(resumen)-1])\n",
    "        resumen.remove(resumen[len(resumen)-1])\n",
    "        resumen.remove(resumen[len(resumen)-1])\n",
    "        resumen.remove(resumen[0])\n",
    "        \n",
    "        n = len(resumen)\n",
    "        \n",
    "        resumen_bueno=[\"\"]\n",
    "        for p in range(n):\n",
    "            resumen_bueno[0]+=resumen[p]\n",
    "            \n",
    "        resu.append(resumen_bueno)\n",
    "        w+=1\n",
    "        d1=(w/(len(d[str(ur)])))*100\n",
    "        if ur == 'urls1':\n",
    "            print(\"Completado: {} por ciento\".format(round(d1,2)))\n",
    "        else:\n",
    "            pass\n",
    "    return resu, nombre\n",
    "\n",
    "for i in d:\n",
    "    t = threading.Thread(target=worker, args=(i,))\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "    \n",
    "dic = dict()\n",
    "for j in threads:\n",
    "    j.join()\n",
    "    for di in range(len(nombre)):\n",
    "        dic[nombre[di]]=resu[di]\n",
    "\n",
    "print(\"| Tiempo de scrapeo total: %0.4f s\" % (time() - t00),\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos guardar los resumenes en un archivo csv. Aunque es opcional, es recomendable debido a que así ya no tendremos que escrapear los resumenes nuevamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resu_lib = open('ResumenesLibros.csv', 'w', newline='', encoding='utf-8')\n",
    "salida = csv.writer(resu_lib)\n",
    "for i in dic:\n",
    "    salida.writerow([i, dic[i]])\n",
    "del salida\n",
    "resu_lib.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora comenzaremos a limpiar los resumenes, ya que al bajarnos el código HTML suele venir con algunos símbolos. También seleccionaremos el tipo de separación que vamos a hacer (tokenizar, por palabras) y el idioma de los \"StopWords\". Los stop words son palabras que se suelen repetir mucho y que no nos dice nada del libro, como las conjunciones, artículos etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('spanish'))\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"spanish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos los resumenes del fichero generado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leerLibros(n_max):\n",
    "    data = open('ResumenesLibros.csv', 'r', encoding='utf-8')\n",
    "    resu_lib = csv.reader(data, delimiter=',')\n",
    "    books = {}\n",
    "    n = 0\n",
    "    for row in resu_lib:\n",
    "        books[str(row[0])]=row[1]\n",
    "        if n > n_max:\n",
    "            break\n",
    "        n += 1\n",
    "    data.close()\n",
    "    return books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, crearemos las funciones que nos hará el preprocesado y limpieza de los resumenes de los libros. Para ello, recorremos todas las oraciones de un texto, que son los resumenes de los libros.\n",
    "\n",
    "* nltk.word_tokenize devuelve la lista de palabras que forman la frase (tokenización).\n",
    "\n",
    "* nltk.pos_tag devuelve el part of speech (categoría) correspondiente a la palabra introducida.\n",
    "\n",
    "* nltk.ne_chunk devuelve la etiqueta correspondiente al part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nombres = []\n",
    "def obtenerNombresPropios(nombres, texto):\n",
    "\n",
    "    for frase in nltk.sent_tokenize(texto):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(frase))):\n",
    "            try:\n",
    "                if chunk.label() == 'PERSON':\n",
    "                    for c in chunk.leaves():\n",
    "                        if str(c[0].lower()) not in nombres:\n",
    "                            nombres.append(str(c[0]).lower())\n",
    "            except AttributeError:\n",
    "                pass\n",
    "    return nombres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ahora eliminaremos los signos de puntuación usando tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocesarLibros(libros):\n",
    "    print(\"Preprocesando libros\")\n",
    "    nombresPropios = []\n",
    "\n",
    "    for elemento in libros:\n",
    "\n",
    "        libro = elemento\n",
    "\n",
    "        resumen = libros[elemento]\n",
    "        texto = ' '.join(tokenizer.tokenize(resumen))\n",
    "\n",
    "        nombresPropios = obtenerNombresPropios(nombresPropios, texto)\n",
    "\n",
    "    ignoraPalabras = stopWords\n",
    "    ignoraPalabras.union(nombresPropios)\n",
    "\n",
    "    palabras = [[]]\n",
    "    for elemento in libros:\n",
    "        libro = elemento\n",
    "        textoPreprocesado = []\n",
    "        for palabra in tokenizer.tokenize(texto):\n",
    "            if (palabra.lower() not in ignoraPalabras):\n",
    "                textoPreprocesado.append(stemmer.stem(palabra.lower()))\n",
    "                palabras.append([(stemmer.stem(palabra.lower()))])\n",
    "\n",
    "    return palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez preprocesado el texto de los resuemesn podemos crear la colección de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crearColeccionTextos(libros):\n",
    "    print(\"Creando colección global de resúmenes\")\n",
    "    textos = []\n",
    "    \n",
    "    for elemento in libros:\n",
    "        libro = elemento\n",
    "        texto = libros[elemento]\n",
    "        lista = texto.split(' ')\n",
    "\n",
    "        textos.append(lista)\n",
    "\n",
    "    return textos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora crearemos el diccionario de palabras, que estará formado por la concatenación de todas las palabras que aparecen en el resumen de algun  libro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crearDiccionario(textos):\n",
    "    print(\"Creación del diccionario global\")\n",
    "    return corpora.Dictionary(textos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora crearemos el corpus de los resúmenes preprocesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crearCorpus(diccionario):\n",
    "    print(\"Creación del corpus global con los resúmenes de todos los libros\")\n",
    "    return [diccionario.doc2bow(texto) for texto in textos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación vamos a seleccionar el número de libros que queramos tratar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocesando libros\n",
      "Creando colección global de resúmenes\n",
      "Creación del diccionario global\n",
      "Creación del corpus global con los resúmenes de todos los libros\n"
     ]
    }
   ],
   "source": [
    "libros = leerLibros(50)\n",
    "palabras = preprocesarLibros(libros)\n",
    "textos = crearColeccionTextos(libros)\n",
    "diccionario = crearDiccionario(textos)\n",
    "corpus = crearCorpus(diccionario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación del modelo TF-IDF (https://es.wikipedia.org/wiki/Tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crearTfIdf(corpus):\n",
    "    print(\"Creación del Modelo Espacio-Vector Tf-Idf\")\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    return corpus_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Creación del modelo LSA (Latent Semantic Analysis). Añadiremos unos parámetros de control del recomendador en si."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TOTAL_TOPICOS_LSA = 30\n",
    "UMBRAL_SIMILITUD = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crearLSA(corpus,pel_tfidf):\n",
    "    print(\"Creación del modelo LSA: Latent Semantic Analysis\")\n",
    "    numpy_matrix = gensim.matutils.corpus2dense(corpus,\n",
    "                                                num_terms = 50000)\n",
    "    svd = np.linalg.svd(numpy_matrix, full_matrices=False,\n",
    "                        compute_uv = False)\n",
    "\n",
    "    lsi = models.LsiModel(pel_tfidf, id2word=diccionario,\n",
    "                          num_topics=TOTAL_TOPICOS_LSA)\n",
    "\n",
    "    indice = similarities.MatrixSimilarity(lsi[pel_tfidf])\n",
    "\n",
    "    return lsi,indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crearNombresLibros(libros):\n",
    "    nombresLibros = []\n",
    "    for elemento in libros:\n",
    "        nombresLibros.append(elemento)\n",
    "    return nombresLibros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos el modelo de similitud donde se aplicará el umbral anteriormente seleccionado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crearModeloSimilitud(libros, pel_tfidf,lsi,indice):\n",
    "    nombreLibros = crearNombresLibros(libros)\n",
    "    print(\"Creando enlaces de similitud entre libros\")\n",
    "    print('')\n",
    "    print('Recomendaciones:')\n",
    "    print('')\n",
    "    similar = []\n",
    "    for i, doc in enumerate(pel_tfidf):\n",
    "        libroI = nombreLibros[i]\n",
    "            \n",
    "        vec_lsi = lsi[doc]\n",
    "        indice_similitud = indice[vec_lsi]\n",
    "        similares = []\n",
    "        for j, elemento in enumerate(libros):\n",
    "            s = indice_similitud[j]\n",
    "            libroJ = j\n",
    "            if (s > UMBRAL_SIMILITUD) and (nombreLibros[i] != nombreLibros[j]):\n",
    "                similar.append(nombreLibros[j])\n",
    "                if nombreLibros[i] in similar:\n",
    "                    pass\n",
    "                else:\n",
    "                    print('Libro:','|',libroI.upper(),'|',round(s,3),\n",
    "                          \"similitud ->\",'|',nombreLibros[j].upper(),'|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, ya estamos listos para ver que libros \"se parecen\" a otros libros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creación del Modelo Espacio-Vector Tf-Idf\n",
      "Creación del modelo LSA: Latent Semantic Analysis\n",
      "Creando enlaces de similitud entre libros\n",
      "\n",
      "Recomendaciones:\n",
      "\n",
      "Libro: | EL SEÑOR DE LOS ANILLOS II - LAS DOS TORRES  | 0.976 similitud -> | PATAS ARRIBA  |\n",
      "Libro: | LOS MITOS DE LA HISTORIA ARGENTINA 2  | 0.866 similitud -> | LOS MITOS DE LA HISTORIA ARGENTINA 3  |\n",
      "Libro: | LOS MITOS DE LA HISTORIA ARGENTINA 2  | 0.929 similitud -> | LOS MITOS DE LA HISTORIA ARGENTINA 5  |\n",
      "Libro: | LOS MITOS DE LA HISTORIA ARGENTINA 2  | 0.933 similitud -> | LOS MITOS DE LA HISTORIA ARGENTINA  |\n",
      "Libro: | EL GRAN DISEÑO  | 0.869 similitud -> | STEPHEN HAWKING: SU VIDA Y OBRA  |\n",
      "Libro: | HISTORIAS DE DIVÁN  | 0.939 similitud -> | PALABRAS CRUZADAS  |\n",
      "Libro: | FARENHEIT 451  | 0.991 similitud -> | EL VINO DEL ESTÍO  |\n",
      "Libro: | EL TRIUNFO DEL SOL  | 0.978 similitud -> | LA RUTA DE LOS VENGADORES  |\n",
      "Libro: | EL AMOR EN LOS TIEMPOS DEL CÓLERA  | 0.954 similitud -> | EL GENERAL EN SU LABERINTO  |\n",
      "Libro: | CAVERNAS Y PALACIOS  | 0.877 similitud -> | SEXO, DROGAS Y BIOLOGÍA  |\n",
      "Libro: | ENSAYO SOBRE LA LUCIDEZ  | 0.965 similitud -> | LA BALSA DE PIEDRA  |\n"
     ]
    }
   ],
   "source": [
    "pel_tfidf   = crearTfIdf(corpus)\n",
    "(lsi,indice)= crearLSA(corpus,pel_tfidf)\n",
    "crearModeloSimilitud(libros,pel_tfidf,lsi,indice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
